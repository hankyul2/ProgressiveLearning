{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATIC MIXED PRECISION\n",
    "\n",
    "- `torch.cuda.amp`는 Automatic Mixed Precision (AMP) 를 제공한다. \n",
    "- AMP란 특정 계산은 `torch.float32`(`float`) 으로 하고 몇몇 계산은 `torch.float16` (`half`)로 하는 것을 뜻 한다.  \n",
    "- 이 노트북에서는 간단한 모델을 학습해보고 해당 모델에 `torch.cuda.amp.autocast` 와 `torch.cuda.amp.GradScaler`를 직접 적용해 보는 방법을 학습한다.\n",
    "- 해당 노트북을 돌리기 위해서 설치해야하는 것은 파이썬과 파이토치다.\n",
    "- AMP의 효과를 최대로 보기 위해서는 Tensor Core-enabled Architecture (Volta, Turing, Ampere)로 설계된 GPU를 사용하는 것이 좋다. 예) RTX 3090 (Ampere)\n",
    "\n",
    "Background\n",
    "\n",
    "- Pytorch\n",
    "- `gc.collect`: [docs](https://docs.python.org/3/library/gc.html#gc.collect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time, torch, os\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6,'\n",
    "\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + msg)\n",
    "    print(\"Total Execution Time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(in_size, out_size, num_layers):\n",
    "    layers = []\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(in_size, in_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(in_size, out_size))\n",
    "    return torch.nn.Sequential(*layers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "in_size = 4096\n",
    "out_size = 4096\n",
    "num_layers = 3\n",
    "num_batches = 50\n",
    "epochs = 3\n",
    "\n",
    "data = [torch.randn(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\n",
    "targets = [torch.randn(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:274: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total Execution Time = 2.532 sec\n",
      "Max memory used by tensors = 1350681600 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "start_timer()\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "end_timer_and_print(\"Default precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py:274: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed precision:\n",
      "Total Execution Time = 0.840 sec\n",
      "Max memory used by tensors = 1577232896 bytes\n"
     ]
    }
   ],
   "source": [
    "use_amp = True\n",
    "\n",
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "start_timer()\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad()\n",
    "\n",
    "end_timer_and_print(\"Mixed precision:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
